{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pickle, os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import requests\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "import torch\n",
    "import gc, sys, psutil\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./videodatainfo_2017_ustc.json', 'r') as f:\n",
    "    parsed = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['info', 'videos', 'sentences'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caption</th>\n",
       "      <th>sen_id</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a cartoon animals runs through an ice cave in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>video2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a cartoon character runs around inside of a vi...</td>\n",
       "      <td>1</td>\n",
       "      <td>video2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a character is running in the snow</td>\n",
       "      <td>2</td>\n",
       "      <td>video2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a person plays a video game centered around ic...</td>\n",
       "      <td>3</td>\n",
       "      <td>video2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a person plays online and records themselves</td>\n",
       "      <td>4</td>\n",
       "      <td>video2960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             caption  sen_id   video_id\n",
       "0  a cartoon animals runs through an ice cave in ...       0  video2960\n",
       "1  a cartoon character runs around inside of a vi...       1  video2960\n",
       "2                 a character is running in the snow       2  video2960\n",
       "3  a person plays a video game centered around ic...       3  video2960\n",
       "4       a person plays online and records themselves       4  video2960"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(parsed.keys())\n",
    "data = pd.DataFrame(parsed['sentences'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Video(object):\n",
    "    \n",
    "    def __init__(self, video_id, captions=[], sentence_ids=[]):\n",
    "        self.video_id = video_id\n",
    "        self.captions = captions\n",
    "        self.sentence_ids = sentence_ids\n",
    "        \n",
    "    def captions_to_token(self, vocab_to_int):\n",
    "        \n",
    "        self.caption_tokens = []\n",
    "        for caption in self.captions:\n",
    "            ids = []\n",
    "            words = caption.split()\n",
    "            for word in words:\n",
    "                ids.append(vocab_to_int[word])\n",
    "\n",
    "            self.caption_tokens.append(ids)\n",
    "        \n",
    "        return self.caption_tokens\n",
    "    \n",
    "    \n",
    "    def pad_captions(self, seq_len):\n",
    "        \n",
    "        self.padded_captions = []\n",
    "        for ids in self.caption_tokens:\n",
    "            ids = np.array(ids)\n",
    "            if len(ids) > seq_len:\n",
    "                ids = ids[:seq_len]\n",
    "            else:\n",
    "                ids = np.pad(ids,(0,seq_len-len(ids)),'constant')\n",
    "\n",
    "            self.padded_captions.append(list(ids))\n",
    "        \n",
    "        return self.padded_captions\n",
    "\n",
    "    \n",
    "    def calculate_output_captions(self):\n",
    "        \n",
    "        self.output_captions = []\n",
    "        \n",
    "        for ids in self.padded_captions:\n",
    "            ids = ids[1:]\n",
    "            ids.append(0)\n",
    "            \n",
    "            self.output_captions.append(ids)\n",
    "        \n",
    "        return self.output_captions\n",
    "    \n",
    "    def get_vgg_tensor(self, transforms, model):\n",
    "    \n",
    "        vgg_features = []\n",
    "        path = './frames/'+self.video_id+'/'\n",
    "        frames = os.listdir(path)\n",
    "        for frame in frames:\n",
    "            img = Image.open(path+frame)\n",
    "            img = transforms(img)\n",
    "\n",
    "            #features = model(img.unsqueeze(0))\n",
    "\n",
    "            vgg_features.append(img)\n",
    "\n",
    "        feats = torch.stack(vgg_features, dim=0)\n",
    "        feats = feats.cuda()\n",
    "        model = model.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.vgg_tensor = model(feats)\n",
    "\n",
    "        del feats\n",
    "        gc.collect()\n",
    "        model = model.cpu()\n",
    "        self.vgg_tensor = vgg_tensor.cpu()\n",
    "\n",
    "        return self.vgg_tensor\n",
    "\n",
    "    \n",
    "    def __str__(self):\n",
    "        return '{}'.format(self.video_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "    def forward(self, x):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vgg_tensor(video_id, transforms, model):\n",
    "    \n",
    "    vgg_features = []\n",
    "    path = './frames/'+video_id+'/'\n",
    "    frames = os.listdir(path)\n",
    "    for frame in frames:\n",
    "        img = Image.open(path+frame)\n",
    "        img = transforms(img)\n",
    "        \n",
    "        #features = model(img.unsqueeze(0))\n",
    "        \n",
    "        vgg_features.append(img)\n",
    "    \n",
    "    feats = torch.stack(vgg_features, dim=0)\n",
    "    feats = feats.cuda()\n",
    "    model = model.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        vgg_tensor = model(feats)\n",
    "    \n",
    "    del feats\n",
    "    gc.collect()\n",
    "    model = model.cpu()\n",
    "    vgg_tensor = vgg_tensor.cpu()\n",
    "\n",
    "    return vgg_tensor\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms_vgg():\n",
    "    transform = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
    "    \n",
    "    vgg = models.vgg16(pretrained=True)\n",
    "    vgg.classifier = nn.Sequential(*[vgg.classifier[i] for i in range(4)])\n",
    "    \n",
    "    return transform, vgg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform, model = get_transforms_vgg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = get_vgg_tensor('video0', transform, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memReport():\n",
    "    for obj in gc.get_objects():\n",
    "        if torch.is_tensor(obj):\n",
    "            print(type(obj), obj.size())\n",
    "    \n",
    "def cpuStats():\n",
    "        print(sys.version)\n",
    "        print(psutil.cpu_percent())\n",
    "        print(psutil.virtual_memory())  # physical memory usage\n",
    "        pid = os.getpid()\n",
    "        py = psutil.Process(pid)\n",
    "        memoryUse = py.memory_info()[0] / 2. ** 30  # memory use in GB...I think\n",
    "        print('memory GB:', memoryUse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.3 (v3.6.3:2c5fed8, Oct  3 2017, 18:11:49) [MSC v.1900 64 bit (AMD64)]\n",
      "4.4\n",
      "svmem(total=17046654976, available=7899828224, percent=53.7, used=9146826752, free=7899828224)\n",
      "memory GB: 2.3004684448242188\n"
     ]
    }
   ],
   "source": [
    "cpuStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'> torch.Size([4096, 25088])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([4096])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([4096, 4096])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([4096])\n",
      "<class 'torch.Tensor'> torch.Size([32, 4096])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64, 3, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64, 64, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([128, 64, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([128, 128, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([256, 128, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([256, 256, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([256, 256, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 256, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "memReport()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video_objects():\n",
    "    video_objects = [None] * 1\n",
    "    for i in range(len(video_objects)):\n",
    "\n",
    "        video_id = 'video'+str(i)\n",
    "        captions = list(data[data['video_id'] == video_id].caption)\n",
    "        sentence_ids = list(data[data['video_id'] == video_id].sen_id)\n",
    "\n",
    "        video_objects[i] = Video(video_id=video_id, captions=captions,sentence_ids=sentence_ids)\n",
    "    \n",
    "    return video_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_caption_dict()->tuple:\n",
    "    \n",
    "    captions_dict = {}\n",
    "    sentence_ids = {} \n",
    "\n",
    "    for video_id in list(data['video_id'].unique()):\n",
    "\n",
    "        caption_list = list(data[data['video_id'] == video_id].caption)\n",
    "        sentence_list = list(data[data['video_id'] == video_id].sen_id)\n",
    "        captions[video_id] = caption_list\n",
    "        sentence_ids[video_id] = sentence_list\n",
    "        \n",
    "    return captions_dict, sentence_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('captions.pickle', 'wb') as handle:\n",
    "    pickle.dump(captions, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('captions.pickle', 'rb') as handle:\n",
    "    captions_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_text(captions_dict:dict)->list:\n",
    "\n",
    "    caption_text = []\n",
    "    for k, v in captions_dict.items():\n",
    "        for caption in v:\n",
    "            caption = ''.join([ch for ch in caption if ch not in punctuation])\n",
    "            caption_text.append(caption)\n",
    "    \n",
    "    return caption_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_text = gather_text(captions_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(caption_text):\n",
    "    words = []\n",
    "    for text in caption_text:\n",
    "        for word in text.split():\n",
    "            words.append(word)\n",
    "    \n",
    "    counts = Counter(words)\n",
    "    vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "    vocab_to_int = {word:i for i, word in enumerate(vocab, 1)}\n",
    "    int_to_vocab = {v:k for k,v in vocab_to_int.items()}\n",
    "    \n",
    "    return vocab, vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = create_vocab(caption_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_to_int(caption_text, vocab_to_int):\n",
    "    captions_to_int = []\n",
    "    for i, caption in enumerate(caption_text):\n",
    "        ids = []\n",
    "        words = caption.split()\n",
    "\n",
    "        for word in words:\n",
    "\n",
    "            ids.append(vocab_to_int[word])\n",
    "\n",
    "        captions_to_int.append(ids)\n",
    "    \n",
    "    return captions_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 20\n",
    "def pad_sequences():\n",
    "    padded_captions = []\n",
    "    for ids in captions_to_int:\n",
    "        ids = np.array(ids)\n",
    "        if len(ids) > 20:\n",
    "            ids = ids[:20]\n",
    "        else:\n",
    "            ids = np.pad(ids,(seq_len-len(ids),0),'constant')\n",
    "\n",
    "        padded_captions.append(list(ids))\n",
    "    \n",
    "    return padded_captions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
