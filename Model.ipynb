{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pickle, os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import requests\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "import torch\n",
    "import gc, sys, psutil\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, input_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vgg_model = self.get_vgg()\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=input_dim, hidden_size=hidden_dim)    \n",
    "    \n",
    "    def get_vgg(self):\n",
    "        \n",
    "        vgg = models.vgg16(pretrained=True)\n",
    "        vgg.classifier = nn.Sequential(*[vgg.classifier[i] for i in range(4)])\n",
    "        \n",
    "        return vgg\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x = [bs, seq_len, 3, 224, 224]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            vgg_features = self.vgg_model(x)\n",
    "        \n",
    "        # vgg_features = [seq_len, bs, 4096]\n",
    "        \n",
    "        outputs, hidden = self.gru(vgg_features)\n",
    "        \n",
    "        # outputs = [seq_len, bs, hidden_dim*num_directions] = [32, 32, 512]\n",
    "        # hidden = [num_layers*num_directions, bs, hidden_dim] = [1, 32, 512]\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, output_vocab_dim, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.output_vocab_dim = output_vocab_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=output_vocab_dim, embedding_dim=embedding_dim)\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=embedding_dim, hidden_size=hidden_dim)\n",
    "        \n",
    "        self.linear = nn.Linear(in_features=hidden_dim, out_features=output_vocab_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, inp, hidden):\n",
    "        \n",
    "        # inp = [bs] as decoder produces only one word per time-step\n",
    "        # hidden = [num_layers, bs, hidden_dim]\n",
    "        \n",
    "        inp = inp.unsqueeze(0)\n",
    "        \n",
    "        # inp [1, bs], introduce the seq_len dimension which is 1 since \n",
    "        # only word is produced by the decoder in one fwd pass\n",
    "        \n",
    "        embed = self.dropout(self.embedding(inp))\n",
    "        \n",
    "        # embed = [1, bs, emb_dim]\n",
    "        \n",
    "        outputs, hidden = self.gru(embed, hidden)\n",
    "        \n",
    "        # outputs = [seq_len, bs, hidden_dim*num_directions] = [1, bs, hidden_dim]\n",
    "        # hidden = [num_layers*num_directions, bs, hidden_dim] = [1, bs, hidden_dim]\n",
    "        \n",
    "        prediction = self.linear(outputs.squeeze(0))\n",
    "        \n",
    "        # [bs, decoder_vocab_dim]\n",
    "        \n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src_frames, target, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        # src_frames [src_len, bs] = [32, bs]\n",
    "        # target = [target_len, bs] = [20, bs]\n",
    "        \n",
    "        batch_size = target.shape[1]\n",
    "        target_len = target.shape[0] # although this has been hardcoded to 20 as of now\n",
    "        \n",
    "        target_vocab_size = self.decoder.output_vocab_dim\n",
    "        \n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(self.device)\n",
    "        \n",
    "        # [trg_len, bs, vocab_dim]\n",
    "        \n",
    "        hidden = self.encoder(src_frames)\n",
    "        \n",
    "        # hidden [1, bs, hidden_dim]\n",
    "        \n",
    "        inp = target[0,:]\n",
    "        \n",
    "        # first input to the token is <sos> token. TODO add this in the sentences. \n",
    "        \n",
    "        for i in range(1, target_len):\n",
    "            \n",
    "            decoder_output, hidden = self.decoder(inp)\n",
    "            \n",
    "            outputs[i] = decoder_output\n",
    "            \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            max_prob_word = decoder_output.argmax(1)\n",
    "            \n",
    "            inp = target[i] if teacher_force else max_prob_word\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 4096\n",
    "output_vocab_dim = 21000 #len(vocab) #import vocab here\n",
    "encoder_hid_dim = 512\n",
    "decoder_hid_dim = 512\n",
    "embedding_dim = 256\n",
    "dropout = 0.4\n",
    "\n",
    "encoder = Encoder(hidden_dim=encoder_hid_dim, input_dim=input_dim)\n",
    "decoder = Decoder(embedding_dim, decoder_hid_dim, output_vocab_dim, dropout)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (vgg_model): VGG(\n",
       "      (features): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (8): ReLU(inplace=True)\n",
       "        (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (11): ReLU(inplace=True)\n",
       "        (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (13): ReLU(inplace=True)\n",
       "        (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (15): ReLU(inplace=True)\n",
       "        (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (18): ReLU(inplace=True)\n",
       "        (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (20): ReLU(inplace=True)\n",
       "        (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (22): ReLU(inplace=True)\n",
       "        (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (25): ReLU(inplace=True)\n",
       "        (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (27): ReLU(inplace=True)\n",
       "        (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (29): ReLU(inplace=True)\n",
       "        (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "      (classifier): Sequential(\n",
       "        (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.5, inplace=False)\n",
       "        (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (gru): GRU(4096, 512)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(21000, 256)\n",
       "    (gru): GRU(256, 512)\n",
       "    (linear): Linear(in_features=512, out_features=21000, bias=True)\n",
       "    (dropout): Dropout(p=0.4, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, opt, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0.\n",
    "    \n",
    "    for frames, captions in loader:\n",
    "        \n",
    "        # frames = [16, 32, 3, 224, 224]\n",
    "        # captions = [16, 20, 20]\n",
    "        frames = frames.permute(1, 0, 2, 3, 4)\n",
    "        captions = captions.permute(1, 2, 0)\n",
    "        \n",
    "        for i in range(captions.shape[0]):\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            \n",
    "            output = model(frames, captions[i])\n",
    "            \n",
    "            # output [trg_len, bs, output_vocab_dim]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            \n",
    "            target = captions[i][1:].view(-1)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "            opt.step()\n",
    "        \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    \n",
    "    return epoch_loss / len(loader)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
